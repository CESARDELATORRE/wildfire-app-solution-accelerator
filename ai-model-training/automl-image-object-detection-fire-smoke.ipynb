{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AutoML: Train Image Object Detection model for a 'Wildfire' dataset.\n",
        "\n",
        "**Requirements**\n",
        "- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
        "- An Azure ML workspace. [Check this notebook for creating a workspace](../../../resources/workspace/workspace.ipynb) \n",
        "- A Compute Cluster. [Check this notebook to create a compute cluster](../../../resources/compute/compute.ipynb)\n",
        "- A python environment\n",
        "- Installed Azure Machine Learning Python SDK v2 - [install instructions](../../../README.md) - check the getting started section\n",
        "\n",
        "In this notebook, we go over how you can use AutoML for training an Image Object Detection model. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Connect to Azure Machine Learning Workspace\n",
        "\n",
        "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
        "\n",
        "## 1.1. Import the required libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "from azure.ai.ml.automl import SearchSpace, ObjectDetectionPrimaryMetrics\n",
        "from azure.ai.ml.sweep import (\n",
        "    Choice,\n",
        "    Uniform,\n",
        "    BanditPolicy,\n",
        ")\n",
        "\n",
        "from azure.ai.ml import automl"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1694716640470
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Configure workspace details and get a handle to the workspace\n",
        "\n",
        "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "credential = DefaultAzureCredential()\n",
        "ml_client = None\n",
        "try:\n",
        "    ml_client = MLClient.from_config(credential)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "    # Enter details of your AML workspace\n",
        "    subscription_id = \"<SUBSCRIPTION_ID>\"\n",
        "    resource_group = \"<RESOURCE_GROUP>\"\n",
        "    workspace = \"<AML_WORKSPACE_NAME>\"\n",
        "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1694805744462
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. MLTable with input Training Data\n",
        "\n",
        "In order to generate models for computer vision tasks with automated machine learning, you need to bring labeled image data as input for model training in the form of an MLTable. You can create an MLTable from labeled training data in JSONL format. If your labeled training data is in a different format (like, pascal VOC or COCO), you can use a conversion script to first convert it to JSONL, and then create an MLTable. Alternatively, you can use Azure Machine Learning's [data labeling tool](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-image-labeling-projects) to manually label images, and export the labeled data to use for training your AutoML model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 2.1. Download the Data\n",
        "We first download and unzip the data locally. By default, the data would be downloaded in `./data` folder in current directory. \n",
        "If you prefer to download the data at a different location, update it in `dataset_parent_dir = ...` in the next cell."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# var for data directory\n",
        "dataset_parent_dir = \"./data\"\n",
        "\n",
        "# create data folder if it doesn't exist\n",
        "os.makedirs(dataset_parent_dir, exist_ok=True)\n",
        "\n",
        "# download example data\n",
        "download_url = \"https://public.roboflow.com/ds/APa8TXMiZQ?key=CzkWNCvbeP\"\n",
        "\n",
        "# Name dataset \n",
        "dataset_name = \"AI for Mankind Wildfire Smoke Dataset\"\n",
        "\n",
        "# Get dataset path for later use\n",
        "dataset_dir = os.path.join(dataset_parent_dir, dataset_name)\n",
        "\n",
        "# Get the data zip file path \n",
        "data_file = os.path.join(dataset_parent_dir, f\"{dataset_name}.zip\")\n",
        "\n",
        "# Download the dataset\n",
        "urllib.request.urlretrieve(download_url, filename=data_file)\n",
        "\n",
        "# Extract files\n",
        "with ZipFile(data_file, 'r') as zip:\n",
        "    print(\"extracting files...\")\n",
        "    zip.extractall(path=dataset_parent_dir)\n",
        "    print(\"done\")\n",
        "# Delete zip file\n",
        "os.remove(data_file)\n",
        "\n",
        "dataset_dir = dataset_parent_dir"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694806053728
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a sample image from this dataset:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "sample_image = os.path.join(dataset_dir, \"test\", \"ck0kcoc8ik6ni0848clxs0vif_jpeg.rf.8b4629777ffe1d349cc970ee8af59eac.jpg\")\n",
        "Image(filename=sample_image)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694806061445
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Slimming down data for testing \n",
        "- 7 training images in new directory 'train-slim' \n",
        "- 2 validation images in new directory 'valid-slim'"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import shutil\n",
        "\n",
        "# new slimmed down folder for train data\n",
        "\n",
        "train_dir = os.path.join(dataset_dir,\"train\")\n",
        "\n",
        "new_train_dir = os.path.join(dataset_dir, \"train-slim\")\n",
        "\n",
        "os.makedirs(new_train_dir, exist_ok=True)\n",
        "\n",
        "train_keep = ['ck0k99e6p79go0944lmxivkmv_jpeg.rf.035851b1218826a8d8e1f6cff055da7a.jpg', 'ck0k99e6p79go0944lmxivkmv_jpeg.rf.035851b1218826a8d8e1f6cff055da7a.xml', 'ck0k99r7bir3f0a460bctrlmy_jpeg.rf.0520c5989e9d6326d3beb346e7edc7c2.jpg', 'ck0k99r7bir3f0a460bctrlmy_jpeg.rf.0520c5989e9d6326d3beb346e7edc7c2.xml', 'ck0k9dg0vjxcg0848rmqzl38w_jpeg.rf.aa2243c64fd18ad4e8c179d09c12cbfc.jpg', 'ck0k9dg0vjxcg0848rmqzl38w_jpeg.rf.aa2243c64fd18ad4e8c179d09c12cbfc.xml', 'ck0k9etuqjxhh0848k6i1mw2f_jpeg.rf.65e16bd6d3135e530e8e7677b83b6481.jpg', 'ck0k9etuqjxhh0848k6i1mw2f_jpeg.rf.65e16bd6d3135e530e8e7677b83b6481.xml', 'ck0k9ghqt7a8l0944mcvy0jsx_jpeg.rf.5b55501a403c70b4dba61727046660b5.jpg', 'ck0k9ghqt7a8l0944mcvy0jsx_jpeg.rf.5b55501a403c70b4dba61727046660b5.xml', 'ck0k9jss09oxq0721c8pd7eds_jpeg.rf.fa5ce52e0ebc5762bee66acd5bc1aa1d.jpg', 'ck0k9jss09oxq0721c8pd7eds_jpeg.rf.fa5ce52e0ebc5762bee66acd5bc1aa1d.xml', 'ck0kcaec57i7s0944jkpjyecx_jpeg.rf.0dbf4b042f0b6abc7a9f1a42a82a20dd.jpg', 'ck0kcaec57i7s0944jkpjyecx_jpeg.rf.0dbf4b042f0b6abc7a9f1a42a82a20dd.xml', 'ck0kcbvwl4tja0794csyimfqu_jpeg.rf.9b8fccc32c80221a31a78bc6b8c54b39.jpg', 'ck0kcbvwl4tja0794csyimfqu_jpeg.rf.9b8fccc32c80221a31a78bc6b8c54b39.xml', 'ck0kcju199xgd0721ljl6cd2n_jpeg.rf.1b068071c8dfc4cf3573f0a0e217f523.jpg', 'ck0kcju199xgd0721ljl6cd2n_jpeg.rf.1b068071c8dfc4cf3573f0a0e217f523.xml', 'ck0kck2czj0l10a46f6xkvofl_jpeg.rf.678c741e4261415dc135ecaf38d4581d.jpg', 'ck0kck2czj0l10a46f6xkvofl_jpeg.rf.678c741e4261415dc135ecaf38d4581d.xml']\n",
        "\n",
        "for i in os.listdir(train_dir): \n",
        "    if i in os.listdir(new_train_dir):\n",
        "        continue # skip if already in folder \n",
        "    else: \n",
        "        if i in train_keep:\n",
        "            shutil.copy(os.path.join(train_dir,i), os.path.join(new_train_dir,i))\n",
        "            print(\"copied:\", os.path.join(new_train_dir,i)) \n",
        "\n",
        "train_dir = new_train_dir\n",
        "\n",
        "# new slimmed down folder for valid data\n",
        "\n",
        "valid_dir = os.path.join(dataset_dir,\"valid\")\n",
        "\n",
        "new_valid_dir = os.path.join(dataset_dir, \"valid-slim\")\n",
        "\n",
        "os.makedirs(new_valid_dir, exist_ok=True)\n",
        "\n",
        "valid_keep = ['ck0k9aqm99o2o0721aml8qpqr_jpeg.rf.028c79a871f1964bd02ab8c4b5693e6d.jpg','ck0k9aqm99o2o0721aml8qpqr_jpeg.rf.028c79a871f1964bd02ab8c4b5693e6d.xml', 'ck0k9dzyzirme0a46fhirxayi_jpeg.rf.595063b0fa6a501412d776c8f4d18b77.jpg', \n",
        "'ck0k9dzyzirme0a46fhirxayi_jpeg.rf.595063b0fa6a501412d776c8f4d18b77.xml']\n",
        "\n",
        "for i in os.listdir(valid_dir): \n",
        "    if i in os.listdir(new_valid_dir):\n",
        "        continue # skip if already in folder \n",
        "    else: \n",
        "        if i in valid_keep:\n",
        "            shutil.copy(os.path.join(valid_dir,i), os.path.join(new_valid_dir,i))\n",
        "            print(\"copied:\", os.path.join(new_valid_dir,i)) \n",
        "\n",
        "valid_dir = new_valid_dir\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694729973184
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Upload the images to Datastore through an AML Data asset (URI Folder)\n",
        "\n",
        "In order to use the data for training in Azure ML, we upload it to our default Azure Blob Storage of our  Azure ML Workspace.\n",
        "\n",
        "[Check this notebook for AML data asset example](../../../assets/data/data.ipynb)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading image files by creating a 'data asset URI FOLDER':\n",
        "\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
        "from azure.ai.ml import Input\n",
        "\n",
        "my_data = Data(\n",
        "    path=dataset_dir,\n",
        "    type=AssetTypes.URI_FOLDER,\n",
        "    description=\"AI for Mankind Wildfire Smoke Dataset Pascal VOC\",\n",
        "    name=\"AI-for-Mankind-Wildfire-Smoke-Dataset-Pascal-VOC\",\n",
        ")\n",
        "\n",
        "uri_folder_data_asset = ml_client.data.create_or_update(my_data)\n",
        "\n",
        "print(uri_folder_data_asset)\n",
        "print(\"\")\n",
        "print(\"Path to folder in Blob Storage:\")\n",
        "print(uri_folder_data_asset.path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "upload-data",
        "gather": {
          "logged": 1694806109745
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Convert the downloaded data to JSONL\n",
        "\n",
        "Dataset is annotated in Pascal VOC format. In order to use this data to create an AzureML MLTable, we first need to convert it to the required JSONL format. \n",
        "\n",
        "For documentation on preparing the datasets beyond this notebook, please refer to the [documentation on how to prepare datasets](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-prepare-datasets-for-automl-images)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First generate JSONL files"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The JSONL Conversion helpers require pycocotools and simplification packages"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pycocotools\n",
        "%pip install simplification\n",
        "%pip install scikit-image"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"./jsonl-conversion/\")\n",
        "from base_jsonl_converter import write_json_lines\n",
        "from voc_jsonl_converter import VOCJSONLConverter\n",
        "\n",
        "base_url = uri_folder_data_asset.path\n",
        "\n",
        "# create annotation file for train data\n",
        "\n",
        "train_dir = os.path.join(dataset_dir,\"train\") \n",
        "train_converter = VOCJSONLConverter(os.path.join(base_url,\"train\"), train_dir)\n",
        "train_jsonl_annotations = os.path.join(train_dir,\"train_annotations__voc.jsonl\")\n",
        "write_json_lines(train_converter, train_jsonl_annotations)\n",
        "\n",
        "# create annotation file for valid data\n",
        "\n",
        "valid_dir = os.path.join(dataset_dir,\"valid\")\n",
        "valid_converter = VOCJSONLConverter(os.path.join(base_url,\"valid\"), valid_dir)\n",
        "valid_jsonl_annotations = os.path.join(valid_dir,\"valid_annotations__voc.jsonl\")\n",
        "write_json_lines(valid_converter, valid_jsonl_annotations)\n",
        "\n",
        "# create annotation file for test data\n",
        "\n",
        "test_dir = os.path.join(dataset_dir,\"test\")\n",
        "test_converter = VOCJSONLConverter(os.path.join(base_url,\"test\"), test_dir)\n",
        "test_jsonl_annotations = os.path.join(test_dir,\"test_annotations__voc.jsonl\")\n",
        "write_json_lines(test_converter, test_jsonl_annotations)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694806408687
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If needed, split into train and validation \n",
        "This dataset has already been split at the source."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing bounding boxes"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade matplotlib"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image as pil_image\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "def plot_ground_truth_boxes(image_file, ground_truth_boxes):\n",
        "    # Display the image\n",
        "    plt.figure()\n",
        "    img_np = mpimg.imread(image_file)\n",
        "    img = pil_image.fromarray(img_np.astype(\"uint8\"), \"RGB\")\n",
        "    img_w, img_h = img.size\n",
        "\n",
        "    fig,ax = plt.subplots(figsize=(12, 16))\n",
        "    ax.imshow(img_np)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    label_to_color_mapping = {}\n",
        "\n",
        "    for gt in ground_truth_boxes:\n",
        "        label = gt[\"label\"]\n",
        "\n",
        "        xmin, ymin, xmax, ymax =  gt[\"topX\"], gt[\"topY\"], gt[\"bottomX\"], gt[\"bottomY\"]\n",
        "        topleft_x, topleft_y = img_w * xmin, img_h * ymin\n",
        "        width, height = img_w * (xmax - xmin), img_h * (ymax - ymin)\n",
        "\n",
        "        if label in label_to_color_mapping:\n",
        "            color = label_to_color_mapping[label]\n",
        "        else:\n",
        "            # Generate a random color. If you want to use a specific color, you can use something like \"red\".\n",
        "            color = np.random.rand(3)\n",
        "            label_to_color_mapping[label] = color\n",
        "\n",
        "        # Display bounding box\n",
        "        rect = patches.Rectangle((topleft_x, topleft_y), width, height,\n",
        "                                 linewidth=2, edgecolor=color, facecolor=\"none\")\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Display label\n",
        "        ax.text(topleft_x, topleft_y - 10, label, color=color, fontsize=20)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_ground_truth_boxes_jsonl(image_file, jsonl_file):\n",
        "    image_base_name = os.path.basename(image_file)\n",
        "    ground_truth_data_found = False\n",
        "    with open(jsonl_file) as fp:\n",
        "        for line in fp.readlines():\n",
        "            line_json = json.loads(line)\n",
        "            filename = line_json[\"image_url\"]\n",
        "            if image_base_name in filename:\n",
        "                ground_truth_data_found = True\n",
        "                plot_ground_truth_boxes(image_file, line_json[\"label\"])\n",
        "                break\n",
        "    if not ground_truth_data_found:\n",
        "        print(\"Unable to find ground truth information for image: {}\".format(image_file))"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694806449565
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_file = \"./data/train/ck0k99e6p79go0944lmxivkmv_jpeg.rf.035851b1218826a8d8e1f6cff055da7a.jpg\"\n",
        "jsonl_file = \"./data/train/train_annotations__voc.jsonl\"\n",
        "\n",
        "plot_ground_truth_boxes_jsonl(image_file, jsonl_file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694806469944
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5. Create MLTable data input\n",
        "Create MLTable data input using the jsonl files created above.\n",
        "\n",
        "For documentation on creating your own MLTable assets for jobs beyond this notebook, please refer to below resources\n",
        "- [MLTable YAML Schema](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-mltable) - covers how to write MLTable YAML, which is required for each MLTable asset.\n",
        "- [Create MLTable data asset](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-data-assets?tabs=Python-SDK#create-a-mltable-data-asset) - covers how to create MLTable data asset. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ml_table_file(filename):\n",
        "    \"\"\"Create ML Table definition\"\"\"\n",
        "\n",
        "    return (\n",
        "        \"paths:\\n\"\n",
        "        \"  - file: ./{0}\\n\"\n",
        "        \"transformations:\\n\"\n",
        "        \"  - read_json_lines:\\n\"\n",
        "        \"        encoding: utf8\\n\"\n",
        "        \"        invalid_lines: error\\n\"\n",
        "        \"        include_path_column: false\\n\"\n",
        "        \"  - convert_column_types:\\n\"\n",
        "        \"      - columns: image_url\\n\"\n",
        "        \"        column_type: stream_info\"\n",
        "    ).format(filename)\n",
        "\n",
        "\n",
        "def save_ml_table_file(output_path, mltable_file_contents):\n",
        "    with open(os.path.join(output_path, \"MLTable\"), \"w\") as f:\n",
        "        f.write(mltable_file_contents)\n"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1694806477237
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and save train mltable\n",
        "train_mltable_file_contents = create_ml_table_file(\n",
        "    os.path.basename(train_jsonl_annotations)\n",
        ")\n",
        "save_ml_table_file(train_dir, train_mltable_file_contents)"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694806478902
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and save valid mltable\n",
        "valid_mltable_file_contents = create_ml_table_file(\n",
        "    os.path.basename(valid_jsonl_annotations)\n",
        ")\n",
        "save_ml_table_file(valid_dir, valid_mltable_file_contents)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694806481266
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and save test mltable\n",
        "test_mltable_file_contents = create_ml_table_file(\n",
        "    os.path.basename(test_jsonl_annotations)\n",
        ")\n",
        "save_ml_table_file(test_dir, test_mltable_file_contents)"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694806481731
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training MLTable defined locally, with local data to be uploaded\n",
        "my_training_data_input = Input(type=AssetTypes.MLTABLE, path=train_dir)\n",
        "\n",
        "# Validation MLTable defined locally, with local data to be uploaded\n",
        "my_validation_data_input = Input(type=AssetTypes.MLTABLE, path=valid_dir)\n",
        "\n",
        "# Test MLTable defined locally, with local data to be uploaded\n",
        "my_test_data_input = Input(type=AssetTypes.MLTABLE, path=test_dir)\n",
        "\n",
        "# WITH REMOTE PATH: If available already in the cloud/workspace-blob-store\n",
        "# my_training_data_input = Input(type=AssetTypes.MLTABLE, path=\"azureml://datastores/workspaceblobstore/paths/vision-classification/train\")\n",
        "# my_validation_data_input = Input(type=AssetTypes.MLTABLE, path=\"azureml://datastores/workspaceblobstore/paths/vision-classification/valid\")"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "name": "data-load",
        "gather": {
          "logged": 1694806482524
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Compute target setup\n",
        "\n",
        "We will need to provide a [Compute Target](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#computes) that will be used for your AutoML model training. AutoML models for image tasks require [GPU SKUs](https://docs.microsoft.com/en-us/azure/virtual-machines/sizes-gpu) such as the ones from the NC, NCv2, NCv3, ND, NDv2 and NCasT4 series. We recommend using the NCsv3-series (with v100 GPUs) for faster training. Using a compute target with a multi-GPU VM SKU will leverage the multiple GPUs to speed up training. Additionally, setting up a compute target with multiple nodes will allow for faster model training by leveraging parallelism, when tuning hyperparameters for your model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "from azure.core.exceptions import ResourceNotFoundError\n",
        "\n",
        "# compute_name = \"gpu-cluster-nc6s\"\n",
        "\n",
        "# updated compute name:\n",
        "compute_name = \"gpu-cluster-nc6s-dedicated\"\n",
        "\n",
        "try:\n",
        "    _ = ml_client.compute.get(compute_name)\n",
        "    print(\"Found existing compute target.\")\n",
        "except ResourceNotFoundError:\n",
        "    print(\"Creating a new compute target...\")\n",
        "    compute_config = AmlCompute(\n",
        "        name=compute_name,\n",
        "        type=\"amlcompute\",\n",
        "        size=\"Standard_NC6s_v3\",\n",
        "        idle_time_before_scale_down=120,\n",
        "        min_instances=0,\n",
        "        max_instances=4,\n",
        "    )\n",
        "    ml_client.begin_create_or_update(compute_config).result()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing compute target.\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1694806487551
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Configure and run the AutoML for Images Object Detection training job\n",
        "\n",
        "AutoML allows you to easily train models for Image Classification, Object Detection & Instance Segmentation on your image data. You can control the model algorithm and hyperparameters to be used, perform a sweep over a manually specified hyperparameter space, or the system can automatically perform a hyperparameter sweep for you.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Automatic hyperparameter sweeping for your models (AutoMode)\n",
        "\n",
        "When using AutoML for Images, we can perform an automatic hyperparameter sweep to find the optimal model (we call this functionality AutoMode). The system will choose a model architecture and values for the learning_rate, number_of_epochs, training_batch_size, etc. based on the number of runs. There is no need to specify the hyperparameter search space, sampling method or early termination policy. A number of runs between 10 and 20 will likely work well on many datasets.\n",
        "\n",
        "AutoMode is triggered by setting `max_trials` to a value greater than 1 in limits and by omitting the hyperparameter space, sampling method and termination policy.\n",
        "\n",
        "The following functions configure AutoML image jobs for automatic sweeps:\n",
        "### image_object_detection() function parameters:\n",
        "The `image_object_detection()` factory function allows user to configure the training job.\n",
        "\n",
        "- `compute` - The compute on which the AutoML job will run. In this example we are using a compute called 'gpu-cluster' present in the workspace. You can replace it any other compute in the workspace.\n",
        "- `experiment_name` - The name of the experiment. An experiment is like a folder with multiple runs in Azure ML Workspace that should be related to the same logical machine learning experiment.\n",
        "- `name` - The name of the Job/Run. This is an optional property. If not specified, a random name will be generated.\n",
        "- `primary_metric` - The metric that AutoML will optimize for model selection.\n",
        "- `target_column_name` - The name of the column to target for predictions. It must always be specified. This parameter is applicable to 'training_data' and 'validation_data'.\n",
        "- `training_data` - The data to be used for training. It should contain both training feature columns and a target column. Optionally, this data can be split for segregating a validation or test dataset. \n",
        "You can use a registered MLTable in the workspace using the format '<mltable_name>:<version>' OR you can use a local file or folder as a MLTable. For e.g Input(mltable='my_mltable:1') OR Input(mltable=MLTable(local_path=\"./data\"))\n",
        "The parameter `training_data` must always be provided.\n",
        "\n",
        "### set_limits() function parameters:\n",
        "This is an optional configuration method to configure limits parameters such as timeouts.\n",
        "\n",
        "- `timeout_minutes` - Maximum amount of time in minutes that the whole AutoML job can take before the job terminates. If not specified, the default job's total timeout is 6 days (8,640 minutes).\n",
        "- `max_trials` - Parameter for maximum number of configurations to sweep. Must be an integer between 1 and 1000. When exploring just the default hyperparameters for a given model algorithm, set this parameter to 1. Default value is 1.\n",
        "- `max_concurrent_trials` - Maximum number of runs that can run concurrently. If not specified, all runs launch in parallel. If specified, must be an integer between 1 and 100. Default value is 1.\n",
        "    NOTE: The number of concurrent runs is gated on the resources available in the specified compute target. Ensure that the compute target has the available resources for the desired concurrency.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# general job parameters\n",
        "exp_name = \"dpv2-image-object-detection-experiment-smoke-full\""
      ],
      "outputs": [],
      "execution_count": 67,
      "metadata": {
        "gather": {
          "logged": 1694721332811
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the AutoML job with the related factory-function.\n",
        "\n",
        "image_object_detection_job = automl.image_object_detection(\n",
        "    compute=compute_name,\n",
        "    experiment_name=exp_name,\n",
        "    training_data=my_training_data_input,\n",
        "    validation_data=my_validation_data_input,\n",
        "    target_column_name=\"label\",\n",
        "    primary_metric=\"mean_average_precision\",\n",
        "    tags={\"my_custom_tag\": \"My custom value\"},\n",
        ")\n",
        "\n",
        "image_object_detection_job.set_limits(\n",
        "    max_trials=10,\n",
        "    max_concurrent_trials=1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 69,
      "metadata": {
        "gather": {
          "logged": 1694721340094
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submitting an AutoML job for Computer Vision tasks\n",
        "Once you've configured your job, you can submit it as a job in the workspace in order to train a vision model using your training dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the AutoML job\n",
        "returned_job = ml_client.jobs.create_or_update(\n",
        "    image_object_detection_job\n",
        ")  # submit the job to the backend\n",
        "\n",
        "print(f\"Created job: {returned_job}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694721382354
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.jobs.stream(returned_job.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694729971650
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPTIONAL: If AutoMode doesn't provide good model\n",
        "\n",
        "Skip to section 5 otherwise\n",
        "--------------------------------------------------------------------------"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Individual runs\n",
        "\n",
        "If AutoMode does not meet your needs, you can launch individual runs to explore model algorithms; we provide sensible default hyperparameters for each algorithm. You can also launch individual runs for the same model algorithm and different hyperparameter combinations. The model algorithm is specified using the model_name parameter. Please refer to the [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models?tabs=CLI-v2#configure-model-algorithms-and-hyperparameters) for the list of supported model algorithms.\n",
        "\n",
        "The following function can be used to configure AutoML jobs for individual runs:\n",
        "### set_training_parameters() function parameters:\n",
        "This is an optional configuration method to configure fixed settings or parameters that don't change during the parameter space sweep. Some of the key parameters of this function are:\n",
        "\n",
        "- `model_name` - The name of the ML algorithm that we want to use in training job. Please refer to this [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models?tabs=CLI-v2#supported-model-algorithms) for supported model algorithm.\n",
        "- `number_of_epochs` - The number of training epochs. It must be positive integer (default value is 15).\n",
        "- `layers_to_freeze` - The number of layers to freeze in model for transfer learning. It must be a positive integer (default value is 0).\n",
        "- `early_stopping` - It enable early stopping logic during training, It must be boolean value (default is True).   \n",
        "- `optimizer` - Type of optimizer to use in training. It must be either sgd, adam, adamw (default is sgd).\n",
        "- `distributed` - It enable distributed training if compute target contain multiple GPUs. It must be boolean value (default is True).\n",
        "\n",
        "If you wish to use the default hyperparameter values for a given algorithm (say `yolov5`), you can specify the job for your AutoML Image runs as follows:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the AutoML job with the related factory-function.\n",
        "\n",
        "image_object_detection_job = automl.image_object_detection(\n",
        "    compute=compute_name,\n",
        "    experiment_name=exp_name,\n",
        "    training_data=my_training_data_input,\n",
        "    validation_data=my_validation_data_input,\n",
        "    target_column_name=\"label\",\n",
        ")\n",
        "\n",
        "# Set limits\n",
        "image_object_detection_job.set_limits(timeout_minutes=60)\n",
        "\n",
        "# Pass the fixed settings or parameters\n",
        "image_object_detection_job.set_training_parameters(model_name=\"yolov5\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the AutoML job\n",
        "returned_job = ml_client.jobs.create_or_update(image_object_detection_job)\n",
        "\n",
        "print(f\"Created job: {returned_job}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.jobs.stream(returned_job.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.1 Individual runs with models from MMDetection (Preview)\n",
        "\n",
        "In addition to the models supported natively by AutoML, you can launch individual runs to explore any model from MMDetection version 2.28.2 that supports object detection. Please refer to this [documentation](https://github.com/open-mmlab/mmdetection/blob/v2.28.2/docs/en/model_zoo.md) for the list of models.\n",
        "\n",
        "While you can use any model from MMDetection to support this task, we have curated a set of models in our registry. We provide a set of sensible default hyperparameters for these models. You can fetch the list of curated models using code snippet below."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "registry_ml_client = MLClient(credential, registry_name=\"azureml\")\n",
        "\n",
        "models = registry_ml_client.models.list()\n",
        "object_detection_models = []\n",
        "for model in models:\n",
        "    try:\n",
        "        model = registry_ml_client.models.get(model.name, label=\"latest\")\n",
        "        if model.tags.get(\"task\", \"\") == \"object-detection\":\n",
        "            object_detection_models.append(model.name)\n",
        "    except Exception as ex:\n",
        "        print(f\"Error while accessing registry model list: {ex}\")\n",
        "\n",
        "object_detection_models"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you wish to try a model (say `vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco`), you can specify the job for your AutoML Image runs as follows:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the AutoML job with the related factory-function.\n",
        "\n",
        "image_object_detection_job = automl.image_object_detection(\n",
        "    compute=compute_name,\n",
        "    experiment_name=exp_name,\n",
        "    training_data=my_training_data_input,\n",
        "    validation_data=my_validation_data_input,\n",
        "    target_column_name=\"label\",\n",
        ")\n",
        "\n",
        "# Set limits\n",
        "image_object_detection_job.set_limits(timeout_minutes=60)\n",
        "\n",
        "# Pass the fixed settings or parameters\n",
        "image_object_detection_job.set_training_parameters(\n",
        "    model_name=\"vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the AutoML job\n",
        "returned_job = ml_client.jobs.create_or_update(image_object_detection_job)\n",
        "\n",
        "print(f\"Created job: {returned_job}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.jobs.stream(returned_job.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Manual hyperparameter sweeping for your model\n",
        "\n",
        "When using AutoML for Images, you can perform a hyperparameter sweep over a defined parameter space to find the optimal model. In this example, we sweep over the hyperparameters for `yolov5` and `fasterrcnn_resnet50_fpn` models, both of which are pretrained on COCO, a large-scale object detection, segmentation, and captioning dataset that contains over 200K labeled images with over 80 label categories, choosing from a range of values for learning_rate, optimizer, lr_scheduler, etc., to generate a model with the optimal 'mean_average_precision'. If hyperparameter values are not specified, then default values are used for the specified algorithm.\n",
        "\n",
        "set_sweep function is used to configure the sweep settings:\n",
        "### set_sweep() parameters:\n",
        "- `sampling_algorithm` - Sampling method to use for sweeping over the defined parameter space. Please refer to this [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models?tabs=SDK-v2#sampling-methods-for-the-sweep) for list of supported sampling methods.\n",
        "- `early_termination` - Early termination policy to end poorly performing runs. If no termination policy is specified, all configurations are run to completion. Please refer to this [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models?tabs=SDK-v2#early-termination-policies) for supported early termination policies.\n",
        "\n",
        "We use Random Sampling to pick samples from this parameter space and try a total of 10 iterations with these different samples, running 2 iterations at a time on our compute target. Please note that the more parameters the space has, the more iterations you need to find optimal models.\n",
        "\n",
        "We leverage the Bandit early termination policy which will terminate poor performing configs (those that are not within 20% slack of the best performing config), thus significantly saving compute resources.\n",
        "\n",
        "For more details on model and hyperparameter sweeping, please refer to the [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the AutoML job with the related factory-function.\n",
        "\n",
        "image_object_detection_job = automl.image_object_detection(\n",
        "    compute=compute_name,\n",
        "    experiment_name=exp_name,\n",
        "    training_data=my_training_data_input,\n",
        "    validation_data=my_validation_data_input,\n",
        "    target_column_name=\"label\",\n",
        "    primary_metric=ObjectDetectionPrimaryMetrics.MEAN_AVERAGE_PRECISION,\n",
        "    tags={\"my_custom_tag\": \"My custom value\"},\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1634852262026
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "image-object-detection-configuration",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set limits\n",
        "image_object_detection_job.set_limits(\n",
        "    timeout_minutes=60,\n",
        "    max_trials=10,\n",
        "    max_concurrent_trials=2,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "limit-settings"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the fixed settings or parameters\n",
        "image_object_detection_job.set_training_parameters(\n",
        "    early_stopping=True, evaluation_frequency=1\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "pass-arguments"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure sweep settings\n",
        "image_object_detection_job.set_sweep(\n",
        "    sampling_algorithm=\"random\",\n",
        "    early_termination=BanditPolicy(\n",
        "        evaluation_interval=2, slack_factor=0.2, delay_evaluation=6\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "sweep-settings"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define search space\n",
        "image_object_detection_job.extend_search_space(\n",
        "    [\n",
        "        SearchSpace(\n",
        "            model_name=Choice([\"yolov5\"]),\n",
        "            learning_rate=Uniform(0.0001, 0.01),\n",
        "            model_size=Choice([\"small\", \"medium\"]),  # model-specific\n",
        "            # image_size=Choice(640, 704, 768),  # model-specific; might need GPU with large memory\n",
        "        ),\n",
        "        SearchSpace(\n",
        "            model_name=Choice([\"fasterrcnn_resnet50_fpn\"]),\n",
        "            learning_rate=Uniform(0.0001, 0.001),\n",
        "            optimizer=Choice([\"sgd\", \"adam\", \"adamw\"]),\n",
        "            min_size=Choice([600, 800]),  # model-specific\n",
        "            # warmup_cosine_lr_warmup_epochs=Choice([0, 3]),\n",
        "        ),\n",
        "    ]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "search-space-settings"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the AutoML job\n",
        "returned_job = ml_client.jobs.create_or_update(\n",
        "    image_object_detection_job\n",
        ")  # submit the job to the backend\n",
        "\n",
        "print(f\"Created job: {returned_job}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1634852267930
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "submit-run",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.jobs.stream(returned_job.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "When doing a hyperparameter sweep, it can be useful to visualize the different configurations that were tried using the HyperDrive UI. You can navigate to this UI by going to the 'Child jobs' tab in the UI of the main automl image job from above, which is the HyperDrive parent run. Then you can go into the 'Trials' tab of this HyperDrive parent run. Alternatively, here below you can see directly the HyperDrive parent run and navigate to its 'Trials' tab:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hd_job = ml_client.jobs.get(returned_job.name + \"_HD\")\n",
        "hd_job"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.1 Manual hyperparameter sweeping for models from MMDetection (Preview)\n",
        "\n",
        "Similar to how you can use any model from MMDetection version 2.28.2 for individual runs, you can also include these models to perform a hyperparameter sweep. You can also choose a combination of models supported natively by [AutoML](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models?tabs=CLI-v2#configure-model-algorithms-and-hyperparameters) and models from [MMDetection](https://github.com/open-mmlab/mmdetection/blob/v2.28.2/docs/en/model_zoo.md).\n",
        "\n",
        "In this example, we sweep over `deformable_detr_twostage_refine_r50_16x2_50e_coco`, `sparse_rcnn_r50_fpn_300_proposals_crop_mstrain_480-800_3x_coco`, and `yolov5`, models choosing from a range of values for learning_rate, model_size, etc., to generate a model with the optimal 'MeanAveragePrecision'.."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the AutoML job with the related factory-function.\n",
        "\n",
        "image_object_detection_job = automl.image_object_detection(\n",
        "    compute=compute_name,\n",
        "    experiment_name=exp_name,\n",
        "    training_data=my_training_data_input,\n",
        "    validation_data=my_validation_data_input,\n",
        "    target_column_name=\"label\",\n",
        "    primary_metric=ObjectDetectionPrimaryMetrics.MEAN_AVERAGE_PRECISION,\n",
        "    tags={\"my_custom_tag\": \"My custom value\"},\n",
        ")\n",
        "\n",
        "# Set limits\n",
        "image_object_detection_job.set_limits(\n",
        "    timeout_minutes=240,\n",
        "    max_trials=10,\n",
        "    max_concurrent_trials=2,\n",
        ")\n",
        "\n",
        "# Configure sweep settings\n",
        "image_object_detection_job.set_sweep(\n",
        "    sampling_algorithm=\"random\",\n",
        "    early_termination=BanditPolicy(\n",
        "        evaluation_interval=2, slack_factor=0.2, delay_evaluation=6\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Define search space\n",
        "image_object_detection_job.extend_search_space(\n",
        "    [\n",
        "        SearchSpace(\n",
        "            model_name=Choice([\"yolov5\"]),\n",
        "            learning_rate=Uniform(0.0001, 0.01),\n",
        "            model_size=Choice([\"small\", \"medium\"]),  # model-specific\n",
        "        ),\n",
        "        SearchSpace(\n",
        "            model_name=Choice(\n",
        "                [\n",
        "                    \"deformable_detr_twostage_refine_r50_16x2_50e_coco\",\n",
        "                    \"sparse_rcnn_r50_fpn_300_proposals_crop_mstrain_480-800_3x_coco\",\n",
        "                ]\n",
        "            ),\n",
        "            learning_rate=Uniform(0.00001, 0.0001),\n",
        "            number_of_epochs=Choice([15, 20]),\n",
        "        ),\n",
        "    ]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the AutoML job\n",
        "returned_job = ml_client.jobs.create_or_update(\n",
        "    image_object_detection_job\n",
        ")  # submit the job to the backend\n",
        "\n",
        "print(f\"Created job: {returned_job}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.jobs.stream(returned_job.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------------------"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Retrieve the Best Trial (Best Model's trial/run)\n",
        "Use the MLFLowClient to access the results (such as Models, Artifacts, Metrics) of a previously completed AutoML Trial."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize MLFlow Client\n",
        "\n",
        "The models and artifacts that are produced by AutoML can be accessed via the MLFlow interface.\n",
        "Initialize the MLFlow client here, and set the backend as Azure ML, via. the MLFlow Client.\n",
        "\n",
        "IMPORTANT, you need to have installed the latest MLFlow packages with:\n",
        "\n",
        "    pip install azureml-mlflow\n",
        "\n",
        "    pip install mlflow"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtain the tracking URI for MLFlow"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "\n",
        "# Obtain the tracking URL from MLClient\n",
        "MLFLOW_TRACKING_URI = ml_client.workspaces.get(\n",
        "    name=ml_client.workspace_name\n",
        ").mlflow_tracking_uri\n",
        "\n",
        "print(MLFLOW_TRACKING_URI)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "get_mlflow_tracking_uri",
        "gather": {
          "logged": 1694731201174
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the MLFLOW TRACKING URI\n",
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "print(f\"\\nCurrent tracking uri: {mlflow.get_tracking_uri()}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "set_mlflow_tracking_uri",
        "gather": {
          "logged": 1694731203247
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlflow.tracking.client import MlflowClient\n",
        "\n",
        "# Initialize MLFlow client\n",
        "mlflow_client = MlflowClient()"
      ],
      "outputs": [],
      "execution_count": 75,
      "metadata": {
        "gather": {
          "logged": 1694731206893
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the AutoML parent Job"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "job_name = returned_job.name\n",
        "\n",
        "# Example if providing an specific Job name/ID\n",
        "# job_name = \"salmon_camel_5sdf05xvb3\"\n",
        "\n",
        "# Get the parent run\n",
        "mlflow_parent_run = mlflow_client.get_run(job_name)\n",
        "\n",
        "print(\"Parent Run: \")\n",
        "print(mlflow_parent_run)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "parent_run",
        "gather": {
          "logged": 1694731211304
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print parent run tags. 'automl_best_child_run_id' tag should be there.\n",
        "print(mlflow_parent_run.data.tags)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694731214924
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the AutoML best child run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best model's child run\n",
        "\n",
        "best_child_run_id = mlflow_parent_run.data.tags[\"automl_best_child_run_id\"]\n",
        "print(f\"Found best child run id: {best_child_run_id}\")\n",
        "\n",
        "best_run = mlflow_client.get_run(best_child_run_id)\n",
        "\n",
        "print(\"Best child run: \")\n",
        "print(best_run)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "best_run",
        "gather": {
          "logged": 1694731220850
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get best model run's metrics\n",
        "Access the results (such as Models, Artifacts, Metrics) of a previously completed AutoML Run."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(best_run.data.metrics, index=[0]).T"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 79,
          "data": {
            "text/plain": "                               0\nlog_loss_train          0.456215\nprecision               0.908500\nlog_loss                0.508074\nrecall                  0.945580\nmean_average_precision  0.933580",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>log_loss_train</th>\n      <td>0.456215</td>\n    </tr>\n    <tr>\n      <th>precision</th>\n      <td>0.908500</td>\n    </tr>\n    <tr>\n      <th>log_loss</th>\n      <td>0.508074</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.945580</td>\n    </tr>\n    <tr>\n      <th>mean_average_precision</th>\n      <td>0.933580</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 79,
      "metadata": {
        "gather": {
          "logged": 1694731224360
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the best model locally\n",
        "Access the results (such as Models, Artifacts, Metrics) of a previously completed AutoML Run."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create local folder\n",
        "local_dir = \"./artifact_downloads\"\n",
        "if not os.path.exists(local_dir):\n",
        "    os.mkdir(local_dir)"
      ],
      "outputs": [],
      "execution_count": 80,
      "metadata": {
        "name": "create_local_dir",
        "gather": {
          "logged": 1694731229428
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download run's artifacts/outputs\n",
        "local_path = mlflow_client.download_artifacts(\n",
        "    best_run.info.run_id, \"outputs\", local_dir\n",
        ")\n",
        "print(f\"Artifacts downloaded in: {local_path}\")\n",
        "print(f\"Artifacts: {os.listdir(local_path)}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "download_model",
        "gather": {
          "logged": 1694731239614
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "mlflow_model_dir = os.path.join(local_dir, \"outputs\", \"mlflow-model\")\n",
        "\n",
        "# Show the contents of the MLFlow model folder\n",
        "os.listdir(mlflow_model_dir)\n",
        "\n",
        "# You should see a list of files such as the following:\n",
        "# ['artifacts', 'conda.yaml', 'MLmodel', 'python_env.yaml', 'python_model.pkl', 'requirements.txt']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694731254994
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Register best model and deploy"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Create managed online endpoint"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        "    ProbeSettings,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "import_endpoint_lib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a unique endpoint name with current datetime to avoid conflicts\n",
        "import datetime\n",
        "\n",
        "online_endpoint_name = \"od-fridge-items-\" + datetime.datetime.now().strftime(\n",
        "    \"%m%d%H%M%f\"\n",
        ")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"this is a sample online endpoint for deploying model\",\n",
        "    auth_mode=\"key\",\n",
        "    tags={\"foo\": \"bar\"},\n",
        ")\n",
        "print(online_endpoint_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "endpoint"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "create_endpoint"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Register best model and deploy"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Register model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"od-fridge-items-mlflow-model\"\n",
        "model = Model(\n",
        "    path=f\"azureml://jobs/{best_run.info.run_id}/outputs/artifacts/outputs/mlflow-model/\",\n",
        "    name=model_name,\n",
        "    description=\"my sample object detection model\",\n",
        "    type=AssetTypes.MLFLOW_MODEL,\n",
        ")\n",
        "\n",
        "# for downloaded file\n",
        "# model = Model(\n",
        "#     path=mlflow_model_dir,\n",
        "#     name=model_name,\n",
        "#     description=\"my sample object detection model\",\n",
        "#     type=AssetTypes.MLFLOW_MODEL,\n",
        "# )\n",
        "\n",
        "registered_model = ml_client.models.create_or_update(model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "register_model"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model.id"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploy"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import OnlineRequestSettings\n",
        "\n",
        "# Setting the request timeout to 90 seconds. Please note that if you use a GPU compute, inference would be faster\n",
        "# and this setting may not be required.\n",
        "req_timeout = OnlineRequestSettings(request_timeout_ms=90000)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "deployment = ManagedOnlineDeployment(\n",
        "    name=\"od-fridge-items-mlflow-deploy\",\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=registered_model.id,\n",
        "    instance_type=\"Standard_DS4_V2\",\n",
        "    instance_count=1,\n",
        "    request_settings=req_timeout,\n",
        "    liveness_probe=ProbeSettings(\n",
        "        failure_threshold=30,\n",
        "        success_threshold=1,\n",
        "        timeout=2,\n",
        "        period=10,\n",
        "        initial_delay=2000,\n",
        "    ),\n",
        "    readiness_probe=ProbeSettings(\n",
        "        failure_threshold=10,\n",
        "        success_threshold=1,\n",
        "        timeout=10,\n",
        "        period=10,\n",
        "        initial_delay=2000,\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "deploy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_deployments.begin_create_or_update(deployment).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "create_deploy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# od fridge items deployment to take 100% traffic\n",
        "endpoint.traffic = {\"od-fridge-items-mlflow-deploy\": 100}\n",
        "ml_client.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "update_traffic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get endpoint details"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the details for online endpoint\n",
        "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
        "\n",
        "# existing traffic details\n",
        "print(endpoint.traffic)\n",
        "\n",
        "# Get the scoring URI\n",
        "print(endpoint.scoring_uri)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the deployment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create request json\n",
        "import base64\n",
        "\n",
        "sample_image = os.path.join(dataset_dir, \"images\", \"1.jpg\")\n",
        "\n",
        "\n",
        "def read_image(image_path):\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "request_json = {\n",
        "    \"input_data\": {\n",
        "        \"columns\": [\"image\"],\n",
        "        \"data\": [base64.encodebytes(read_image(sample_image)).decode(\"utf-8\")],\n",
        "    }\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "create_inference_request"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "request_file_name = \"sample_request_data.json\"\n",
        "\n",
        "with open(request_file_name, \"w\") as request_file:\n",
        "    json.dump(request_json, request_file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "dump_inference_request"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    deployment_name=deployment.name,\n",
        "    request_file=request_file_name,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "invoke_inference"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize detections\n",
        "Now that we have scored a test image, we can visualize the bounding boxes for this image."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "IMAGE_SIZE = (18, 12)\n",
        "plt.figure(figsize=IMAGE_SIZE)\n",
        "img_np = mpimg.imread(sample_image)\n",
        "img = Image.fromarray(img_np.astype(\"uint8\"), \"RGB\")\n",
        "x, y = img.size\n",
        "\n",
        "fig, ax = plt.subplots(1, figsize=(15, 15))\n",
        "# Display the image\n",
        "ax.imshow(img_np)\n",
        "\n",
        "# draw box and label for each detection\n",
        "detections = json.loads(resp)\n",
        "for detect in detections[0][\"boxes\"]:\n",
        "    label = detect[\"label\"]\n",
        "    box = detect[\"box\"]\n",
        "    conf_score = detect[\"score\"]\n",
        "    if conf_score > 0.6:\n",
        "        ymin, xmin, ymax, xmax = (\n",
        "            box[\"topY\"],\n",
        "            box[\"topX\"],\n",
        "            box[\"bottomY\"],\n",
        "            box[\"bottomX\"],\n",
        "        )\n",
        "        topleft_x, topleft_y = x * xmin, y * ymin\n",
        "        width, height = x * (xmax - xmin), y * (ymax - ymin)\n",
        "        print(\n",
        "            f\"{detect['label']}: [{round(topleft_x, 3)}, {round(topleft_y, 3)}, \"\n",
        "            f\"{round(width, 3)}, {round(height, 3)}], {round(conf_score, 3)}\"\n",
        "        )\n",
        "\n",
        "        color = np.random.rand(3)  #'red'\n",
        "        rect = patches.Rectangle(\n",
        "            (topleft_x, topleft_y),\n",
        "            width,\n",
        "            height,\n",
        "            linewidth=3,\n",
        "            edgecolor=color,\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(topleft_x, topleft_y - 10, label, color=color, fontsize=20)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "visualize_detections"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delete the deployment and endopoint"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "delte_endpoint"
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "categories": [
      "SDK v2",
      "sdk",
      "python",
      "jobs",
      "automl-standalone-jobs",
      "automl-image-object-detection-task-fridge-items"
    ]
  },
  "nbformat": 4,
  "nbformat_minor": 4
}